# stickers-attack


# Project Summary


With the widespread application of deep learning models in various fields, especially in facial recognition technology, the threat of adversarial attacks has become increasingly prominent, posing great challenges to security and ethics. This project delves into adversarial stickers as a means of attacking facial recognition models, and focuses on optimizing stickers to achieve misclassification. Our goal is to explore how factors such as sticker design, placement, and size affect the recognition results of the model, thereby improving the effectiveness of adversarial attacks.


# Background


Facial recognition systems are ubiquitous in modern society, with a wide range of applications from security surveillance to social media applications. However, these systems are highly susceptible to adversarial attacks, which are maliciously crafted inputs that cause the model to produce incorrect outputs. Adversarial stickers are inputs specifically designed to mislead facial recognition systems by introducing perturbations in the image that are imperceptible or not obvious to the human eye, thereby causing great damage to the underlying model. These stickers are usually developed based on facial features, exploiting the weaknesses of the model to achieve targeted misclassification. This project aims to further improve the success rate of attacks by developing stickers that are both stealthy and balanced in attack effectiveness.


# Benefits


- System Vulnerability Exploration: This project reveals the vulnerability of current facial recognition models to minor perturbations, which helps to gain a deeper understanding of the model's vulnerabilities.

- Contribution to Defense Strategies: By gaining a deeper understanding of the working mechanism of adversarial stickers, this research can support the development of more powerful defense strategies and models that are more resilient to adversarial inputs.

- **Technical Advances**: By trying different sticker placement parameters (such as size and position), this project provides new insights into adversarial attack strategies, which is expected to increase the sophistication of such methods in academic research.


# Disadvantages


- **Ethical Issues**: The development of adversarial attack techniques may have ethical implications, especially when these techniques are abused in unauthorized environments to circumvent facial recognition systems.

- **Limited Application Scope**: The effectiveness of adversarial stickers depends heavily on specific model architectures or environmental settings (such as lighting, viewing angle, etc.), making it difficult to obtain generally applicable results.

- **Model-Specific Dependencies**: Since adversarial stickers exploit specific vulnerabilities in facial recognition models, the research results may not be generally applicable to other models or to models trained using different datasets or under different conditions.


# Challenges

- **Sticker Parameter Optimization**: Determining the optimal placement, size, and transparency of stickers to ensure maximum model confusion is a complex task. The complexity of facial features and the variability of human features add significant challenges to effectively determining these parameters.

- **Model Retraining and Adaptation**: Modern facial recognition systems are increasingly updated via adversarial training, which makes it more challenging to generate stickers that can successfully bypass detection. The need to continuously adjust attacks in response to updated models is a significant obstacle.